MulMoSenT is a modular framework for experimenting with multimodal sentiment analysis techniques. It includes baseline and proposed models that combine textual, visual, and contextual features to jointly learn and predict sentiment. This repository is well-suited for research, benchmarking, and extension with new datasets or fusion methods.

Installation
1. Clone the repository:
git clone https://github.com/sadia-afroze/MulMoSenT.git
cd MulMoSenT
2. Create and activate virtual environment:
 python3 -m venv venv
 source venv/bin/activate
3. Install dependencies:
pip install -r requirements.txt
4. Train Model: python3 MSA_Proposed.py
